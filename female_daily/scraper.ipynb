{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae78412-4d40-4728-8290-5ebc899ac401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import re\n",
    "import time\n",
    "from datetime import date, timedelta, datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa41852-c228-48a9-b64a-b86269e92d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slug_underscore(s: str) -> str:\n",
    "    s = s.strip().lower().replace(\"-\", \"_\")\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def derive_subfolders_from_url(base_url: str) -> tuple[str, str, str]:\n",
    "    parts = urlparse(base_url).path.strip(\"/\").split(\"/\")\n",
    "    if \"products\" not in parts:\n",
    "        raise ValueError(\"URL does not contain '/products/' segment.\")\n",
    "    i = parts.index(\"products\")\n",
    "    tail = parts[i+1:]\n",
    "    if len(tail) < 3:\n",
    "        raise ValueError(f\"Not enough segments after /products/: {tail}\")\n",
    "\n",
    "    cat, brand, product = tail[-4], tail[-2], tail[-1]\n",
    "    return slug_underscore(cat), slug_underscore(brand), slug_underscore(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "848ce3be-0e54-4a59-9008-78e0ed576a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS = r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\"\n",
    "\n",
    "def parse_absolute_date(text: str) -> date | None:\n",
    "    m = re.search(r\"\\b(\\d{1,2})\\s+([A-Za-z]{3})\\s+(\\d{4})\\b\", text)\n",
    "    if not m:\n",
    "        return None\n",
    "    dd, mon, yyyy = m.group(1), m.group(2), m.group(3)\n",
    "    try:\n",
    "        return datetime.strptime(f\"{dd} {mon} {yyyy}\", \"%d %b %Y\").date()\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _parse_amount(token: str) -> int:\n",
    "    token = token.lower()\n",
    "    if token in (\"a\", \"an\"):\n",
    "        return 1\n",
    "    return int(token)\n",
    "\n",
    "\n",
    "def parse_relative_to_date(text: str, today: date | None = None) -> date | None:\n",
    "    \"\"\"\n",
    "    Handles:\n",
    "      'a minute ago', 'an hour ago', '2 minutes ago', '5 hours ago', '3 days ago', '10 seconds ago'\n",
    "    Returns a DATE (not datetime).\n",
    "    \"\"\"\n",
    "    if today is None:\n",
    "        today = date.today()\n",
    "\n",
    "    m = re.search(r\"\\b(a|an|\\d+)\\s+(second|minute|hour|day)s?\\s+ago\\b\", text, flags=re.I)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    amount = _parse_amount(m.group(1))\n",
    "    unit = m.group(2).lower()\n",
    "\n",
    "    if unit == \"day\":\n",
    "        return today - timedelta(days=amount)\n",
    "\n",
    "    # seconds/minutes/hours => same calendar date (today)\n",
    "    return today\n",
    "\n",
    "\n",
    "def extract_date(card_text: str, today: date | None = None) -> tuple[str | None, date | None]:\n",
    "    \"\"\"\n",
    "    Returns (date_raw, date_parsed) where date_raw can be:\n",
    "      - '07 Dec 2025'\n",
    "      - '6 days ago'\n",
    "      - 'an hour ago'\n",
    "    \"\"\"\n",
    "    if today is None:\n",
    "        today = date.today()\n",
    "\n",
    "    abs_m = re.search(r\"\\b\\d{1,2}\\s+[A-Za-z]{3}\\s+\\d{4}\\b\", card_text)\n",
    "    if abs_m:\n",
    "        raw = abs_m.group(0)\n",
    "        return raw, parse_absolute_date(raw)\n",
    "\n",
    "    rel_m = re.search(r\"\\b(a|an|\\d+)\\s+(second|minute|hour|day)s?\\s+ago\\b\", card_text, flags=re.I)\n",
    "    if rel_m:\n",
    "        raw = rel_m.group(0)\n",
    "        return raw, parse_relative_to_date(raw, today=today)\n",
    "\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674589cf-a40c-478c-af0b-6848793ea4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_driver(headless: bool = True) -> webdriver.Chrome:\n",
    "    options = Options()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1200,900\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    return webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e134d82b-3271-4d5e-9903-3dc51e03349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url: str, headless: bool = True, timeout_s: int = 30, scroll: bool = True) -> str:\n",
    "    driver = make_driver(headless=headless)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        WebDriverWait(driver, timeout_s).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.list-reviews, div.review-card\"))\n",
    "        )\n",
    "\n",
    "        if scroll:\n",
    "            for _ in range(4):\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(0.8)\n",
    "\n",
    "        return driver.page_source\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702438f3-61c7-49d3-96ad-bb21037a8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review_text(s: str) -> str | None:\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    x = s.strip()\n",
    "\n",
    "    # remove leading absolute date\n",
    "    x = re.sub(rf\"^\\s*\\d{{1,2}}\\s+{MONTHS}\\s+\\d{{4}}\\s+\", \"\", x)\n",
    "\n",
    "    # remove leading relative timestamp ('2 days ago', 'an hour ago', '5 minutes ago', etc.)\n",
    "    x = re.sub(r\"^\\s*(?:a|an|\\d+)\\s+(?:second|minute|hour|day)s?\\s+ago\\s+\", \"\", x, flags=re.I)\n",
    "\n",
    "    # remove leading \"username recommends/doesn't recommend this product!\"\n",
    "    x = re.sub(\n",
    "        r\"^\\s*\\S+\\s+(?:doesn[’']?t\\s+)?recommends?\\s+this\\s+product!\\s*\",\n",
    "        \"\",\n",
    "        x,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # cut off metadata if it leaks into the wrapper\n",
    "    x = re.split(r\"\\bUsage Period\\b\\s*:\", x, maxsplit=1, flags=re.IGNORECASE)[0]\n",
    "    x = re.split(r\"\\bPurchase Point\\b\\s*:\", x, maxsplit=1, flags=re.IGNORECASE)[0]\n",
    "\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x or None\n",
    "\n",
    "\n",
    "def clean_purchase_point(s: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Removes trailing numbers and extra spaces:\n",
    "      'Shopee 0 0' -> 'Shopee'\n",
    "      'Alfamart 0' -> 'Alfamart'\n",
    "      'Female Daily Studio 0 0' -> 'Female Daily Studio'\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return None\n",
    "    x = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # remove trailing numeric tokens\n",
    "    x = re.sub(r\"(?:\\s+\\d+)+\\s*$\", \"\", x).strip()\n",
    "\n",
    "    return x or None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9755ce-bc50-4a43-9963-882b88c1952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CARD_SELECTOR = \"div.review-card\"\n",
    "\n",
    "def parse_reviews(html: str, card_selector: str = CARD_SELECTOR, today: date | None = None) -> list[dict]:\n",
    "    if today is None:\n",
    "        today = date.today()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    cards = soup.select(card_selector)\n",
    "\n",
    "    rows: list[dict] = []\n",
    "\n",
    "    for card in cards:\n",
    "        card_text = card.get_text(\" \", strip=True)\n",
    "\n",
    "        is_recommended = card.select_one(\"p.recommend\") is not None\n",
    "\n",
    "        date_raw, date_parsed = extract_date(card_text, today=today)\n",
    "\n",
    "        # username heuristic\n",
    "        username = None\n",
    "        for a in card.select(\"a\"):\n",
    "            t = a.get_text(strip=True)\n",
    "            if t and len(t) <= 30:\n",
    "                username = t\n",
    "                break\n",
    "\n",
    "        # rating\n",
    "        rating = None\n",
    "        stars_full = card.select(\"i.icon-ic_big_star_full, i[class*='star'][class*='full'], i[class*='star_full']\")\n",
    "        if stars_full:\n",
    "            rating = len(stars_full)\n",
    "\n",
    "        # usage period\n",
    "        usage_period = None\n",
    "        m = re.search(r\"Usage Period\\s*:\\s*([^:]+?)(?:Purchase Point|$)\", card_text, flags=re.I)\n",
    "        if m:\n",
    "            usage_period = m.group(1).strip()\n",
    "\n",
    "        # purchase point (raw from card text)\n",
    "        purchase_point_raw = None\n",
    "        m = re.search(r\"Purchase Point\\s*:\\s*(.+)$\", card_text, flags=re.I)\n",
    "        if m:\n",
    "            purchase_point_raw = m.group(1).strip()\n",
    "\n",
    "        purchase_point = clean_purchase_point(purchase_point_raw) if purchase_point_raw else None\n",
    "\n",
    "        # review text\n",
    "        content = card.select_one(\"div.review-content-wrapper\")\n",
    "        raw_review = content.get_text(\" \", strip=True) if content else card_text\n",
    "        review = clean_review_text(raw_review)\n",
    "\n",
    "        if review is None:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"date\": date_parsed.isoformat() if date_parsed else None,\n",
    "            \"date_raw\": date_raw,\n",
    "            \"username\": username,\n",
    "            \"review\": review,\n",
    "            \"average_rating\": rating,\n",
    "            \"is_recommended\": is_recommended,\n",
    "            \"usage_period\": usage_period,\n",
    "            \"purchase_point\": purchase_point,\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4979b5a-b42d-4b1e-ba9c-f943a33ef4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ScrapeConfig:\n",
    "    base_url: str\n",
    "    headless: bool = True\n",
    "    timeout_s: int = 30\n",
    "    scroll: bool = True\n",
    "    polite_sleep_s: float = 1.0\n",
    "    card_selector: str = CARD_SELECTOR\n",
    "\n",
    "\n",
    "def scrape(cfg: ScrapeConfig, max_pages: int = 200) -> pd.DataFrame:\n",
    "    all_rows: list[dict] = []\n",
    "    seen = set()\n",
    "\n",
    "    for p in range(1, max_pages + 1):\n",
    "        print(f\"Scraping page {p}...\")\n",
    "        url = f\"{cfg.base_url}?page={p}\"\n",
    "        html = get_html(url, headless=cfg.headless, timeout_s=cfg.timeout_s, scroll=cfg.scroll)\n",
    "\n",
    "        rows = parse_reviews(html, cfg.card_selector)\n",
    "\n",
    "        if not rows:\n",
    "            print(\"No reviews found — stopping.\")\n",
    "            break\n",
    "\n",
    "        new_rows = []\n",
    "        for r in rows:\n",
    "            key = (r.get(\"date\"), r.get(\"username\"), r.get(\"review\"))\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                new_rows.append(r)\n",
    "\n",
    "        if not new_rows:\n",
    "            print(\"No new reviews — stopping.\")\n",
    "            break\n",
    "\n",
    "        all_rows.extend(new_rows)\n",
    "        time.sleep(cfg.polite_sleep_s)\n",
    "\n",
    "    return pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc6f721-6681-4b7d-a080-03a34cce6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PRODUCTS_DIR = Path(\"products\")\n",
    "ROOT_PRODUCTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ae04502-9749-44d5-97c7-a4f361bec62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>date_raw</th>\n",
       "      <th>username</th>\n",
       "      <th>review</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>is_recommended</th>\n",
       "      <th>usage_period</th>\n",
       "      <th>purchase_point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-17</td>\n",
       "      <td>an hour ago</td>\n",
       "      <td>alphandro</td>\n",
       "      <td>masih terjangkau. harga masuk akal gak overpri...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>3 months - 6 months</td>\n",
       "      <td>Alfamart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-15</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>miss_cullen</td>\n",
       "      <td>review jujur,awalnya coba aja,tapi ternyata ba...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1 week - 1 month</td>\n",
       "      <td>Shopee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>carmennita_</td>\n",
       "      <td>Simple dan oke, bisa membersihkan wajah dengan...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>1 month - 3 months</td>\n",
       "      <td>Gift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-07</td>\n",
       "      <td>07 Dec 2025</td>\n",
       "      <td>wulanroshinta</td>\n",
       "      <td>sangat worth to price dan membuat wajah halus ...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>More than 1 year</td>\n",
       "      <td>Shopee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-07</td>\n",
       "      <td>07 Dec 2025</td>\n",
       "      <td>miftanjnh</td>\n",
       "      <td>Facewash Hada Labo tuh lembut banget di kulit,...</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>More than 1 year</td>\n",
       "      <td>Shopee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     date_raw       username  \\\n",
       "0  2025-12-17  an hour ago      alphandro   \n",
       "1  2025-12-15   2 days ago    miss_cullen   \n",
       "2  2025-12-11   6 days ago    carmennita_   \n",
       "3  2025-12-07  07 Dec 2025  wulanroshinta   \n",
       "4  2025-12-07  07 Dec 2025      miftanjnh   \n",
       "\n",
       "                                              review  average_rating  \\\n",
       "0  masih terjangkau. harga masuk akal gak overpri...               5   \n",
       "1  review jujur,awalnya coba aja,tapi ternyata ba...               5   \n",
       "2  Simple dan oke, bisa membersihkan wajah dengan...               5   \n",
       "3  sangat worth to price dan membuat wajah halus ...               5   \n",
       "4  Facewash Hada Labo tuh lembut banget di kulit,...               5   \n",
       "\n",
       "   is_recommended         usage_period purchase_point  \n",
       "0            True  3 months - 6 months       Alfamart  \n",
       "1            True     1 week - 1 month         Shopee  \n",
       "2            True   1 month - 3 months           Gift  \n",
       "3            True     More than 1 year         Shopee  \n",
       "4            True     More than 1 year         Shopee  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL = \"https://reviews.femaledaily.com/products/cleanser/facial-wash/hada-labo/gokyujyun-ultimate-moisturizing-face-wash\"\n",
    "\n",
    "cat, brand, product = derive_subfolders_from_url(BASE_URL)\n",
    "out_dir = ROOT_PRODUCTS_DIR / cat / brand / product\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg = ScrapeConfig(\n",
    "    base_url=BASE_URL,\n",
    "    headless=True,\n",
    "    scroll=True,\n",
    "    card_selector=CARD_SELECTOR,\n",
    ")\n",
    "\n",
    "df = scrape(cfg, max_pages=5)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dcbf190-dc4c-44cb-9795-56d37bf89370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('date_raw', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f34c7f07-5320-4518-997f-6a42e4f99389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('products/cleanser/hada_labo/gokyujyun_ultimate_moisturizing_face_wash/reviews.xlsx')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = out_dir / \"reviews.xlsx\"\n",
    "df.to_excel(out_path, index=False)\n",
    "out_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
